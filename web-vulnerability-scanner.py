import requests
import re
import urllib.parse
import concurrent.futures
import json
import argparse
from typing import List, Dict, Any
from datetime import datetime
from bs4 import BeautifulSoup

class WebVulnerabilityScanner:
    def __init__(self, target_url: str, max_depth: int = 3):
        self.target_url = target_url if target_url.startswith('http') else f'https://{target_url}'
        self.max_depth = max_depth
        self.visited_urls = set()
        self.forms = []
        self.vulnerabilities = []
        self.session = requests.Session()
        
    def extract_links(self, html: str, base_url: str) -> List[str]:
        """Extract all links from HTML content"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = urllib.parse.urljoin(base_url, href)
            if full_url.startswith(self.target_url):
                links.append(full_url)
                
        return list(set(links))

    def extract_forms(self, html: str, url: str) -> List[Dict]:
        """Extract forms and their details from HTML content"""
        soup = BeautifulSoup(html, 'html.parser')
        page_forms = []
        
        for form in soup.find_all('form'):
            form_details = {
                'url': url,
                'action': urllib.parse.urljoin(url, form.get('action', '')),
                'method': form.get('method', 'get').lower(),
                'inputs': []
            }
            
            for input_tag in form.find_all('input'):
                input_type = input_tag.get('type', 'text')
                input_name = input_tag.get('name')
                input_value = input_tag.get('value', '')
                
                if input_name:
                    form_details['inputs'].append({
                        'type': input_type,
                        'name': input_name,
                        'value': input_value
                    })
                    
            page_forms.append(form_details)
        
        return page_forms

    def check_sql_injection(self, url: str, params: Dict) -> None:
        """Check for SQL injection vulnerabilities"""
        sql_payloads = [
            "' OR '1'='1",
            "'; SELECT * FROM users; --",
            "1' OR '1'='1",
            "1; DROP TABLE users--"
        ]
        
        for payload in sql_payloads:
            test_params = params.copy()
            for key in test_params:
                test_params[key] = payload
                
            try:
                response = self.session.get(url, params=test_params)
                if any(error in response.text.lower() for error in 
                    ['sql', 'mysql', 'sqlite', 'postgresql', 'oracle']):
                    self.vulnerabilities.append({
                        'type': 'sql_injection',
                        'url': url,
                        'payload': payload,
                        'parameter': key,
                        'severity': 'High'
                    })
            except:
                continue

    def check_xss(self, url: str, params: Dict) -> None:
        """Check for Cross-Site Scripting (XSS) vulnerabilities"""
        xss_payloads = [
            "<script>alert('xss')</script>",
            "<img src=x onerror=alert('xss')>",
            "javascript:alert('xss')",
            "\"><script>alert('xss')</script>"
        ]
        
        for payload in xss_payloads:
            test_params = params.copy()
            for key in test_params:
                test_params[key] = payload
                
            try:
                response = self.session.get(url, params=test_params)
                if payload in response.text:
                    self.vulnerabilities.append({
                        'type': 'xss',
                        'url': url,
                        'payload': payload,
                        'parameter': key,
                        'severity': 'High'
                    })
            except:
                continue

    def check_open_redirect(self, url: str, params: Dict) -> None:
        """Check for Open Redirect vulnerabilities"""
        redirect_payloads = [
            'https://evil.com',
            '//evil.com',
            '\\evil.com'
        ]
        
        for payload in redirect_payloads:
            test_params = params.copy()
            for key in test_params:
                test_params[key] = payload
                
            try:
                response = self.session.get(url, params=test_params, allow_redirects=False)
                if response.is_redirect and any(payload in x.lower() 
                    for x in [response.headers.get('location', ''), response.text]):
                    self.vulnerabilities.append({
                        'type': 'open_redirect',
                        'url': url,
                        'payload': payload,
                        'parameter': key,
                        'severity': 'Medium'
                    })
            except:
                continue

    def check_file_inclusion(self, url: str, params: Dict) -> None:
        """Check for Local/Remote File Inclusion vulnerabilities"""
        lfi_payloads = [
            '../../../etc/passwd',
            '....//....//....//etc/passwd',
            '/etc/passwd',
            'file:///etc/passwd'
        ]
        
        for payload in lfi_payloads:
            test_params = params.copy()
            for key in test_params:
                test_params[key] = payload
                
            try:
                response = self.session.get(url, params=test_params)
                if any(pattern in response.text for pattern in 
                    ['root:x:', 'bin:', 'daemon:', 'sys:']):
                    self.vulnerabilities.append({
                        'type': 'file_inclusion',
                        'url': url,
                        'payload': payload,
                        'parameter': key,
                        'severity': 'High'
                    })
            except:
                continue

    def crawl(self, url: str, depth: int = 0) -> None:
        """Crawl website to discover content"""
        if depth >= self.max_depth or url in self.visited_urls:
            return
            
        self.visited_urls.add(url)
        print(f"[*] Crawling: {url}")
        
        try:
            response = self.session.get(url)
            new_links = self.extract_links(response.text, url)
            forms = self.extract_forms(response.text, url)
            self.forms.extend(forms)
            
            # Check for vulnerabilities in URL parameters
            parsed_url = urllib.parse.urlparse(url)
            params = dict(urllib.parse.parse_qsl(parsed_url.query))
            if params:
                self.check_sql_injection(url, params)
                self.check_xss(url, params)
                self.check_open_redirect(url, params)
                self.check_file_inclusion(url, params)
            
            # Crawl discovered links
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(self.crawl, link, depth + 1) 
                          for link in new_links]
                concurrent.futures.wait(futures)
                
        except Exception as e:
            print(f"[!] Error crawling {url}: {str(e)}")

    def scan_forms(self) -> None:
        """Test all discovered forms for vulnerabilities"""
        print("[*] Testing discovered forms for vulnerabilities...")
        
        for form in self.forms:
            test_data = {}
            for input_field in form['inputs']:
                if input_field['type'] not in ['submit', 'hidden']:
                    test_data[input_field['name']] = 'test'
            
            if form['method'] == 'post':
                self.check_sql_injection(form['action'], test_data)
                self.check_xss(form['action'], test_data)

    def run_scan(self) -> Dict[str, Any]:
        """Run full vulnerability scan"""
        print(f"[*] Starting vulnerability scan of {self.target_url}")
        start_time = datetime.now()
        
        # Start crawling from target URL
        self.crawl(self.target_url)
        
        # Test discovered forms
        self.scan_forms()
        
        scan_time = datetime.now() - start_time
        
        return {
            'scan_info': {
                'target_url': self.target_url,
                'start_time': start_time.isoformat(),
                'duration': str(scan_time),
                'pages_scanned': len(self.visited_urls),
                'forms_discovered': len(self.forms)
            },
            'vulnerabilities': self.vulnerabilities
        }

    def save_report(self, results: Dict[str, Any], filename: str) -> None:
        """Save scan results to a JSON file"""
        with open(filename, 'w') as f:
            json.dump(results, f, indent=4)
        print(f"[+] Report saved to {filename}")

def main():
    parser = argparse.ArgumentParser(description='Web Application Vulnerability Scanner')
    parser.add_argument('url', help='Target URL to scan')
    parser.add_argument('--depth', type=int, default=3, help='Maximum crawl depth')
    args = parser.parse_args()

    scanner = WebVulnerabilityScanner(args.url, args.depth)
    results = scanner.run_scan()
    
    # Print summary
    print("\n=== Scan Summary ===")
    print(f"Target: {results['scan_info']['target_url']}")
    print(f"Pages scanned: {results['scan_info']['pages_scanned']}")
    print(f"Forms discovered: {results['scan_info']['forms_discovered']}")
    print(f"Vulnerabilities found: {len(results['vulnerabilities'])}")
    
    # Save detailed report
    filename = f"vulnerability_scan_{urllib.parse.urlparse(args.url).netloc}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    scanner.save_report(results, filename)

if __name__ == "__main__":
    main()
